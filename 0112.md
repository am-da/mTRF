

脳波と特徴量の時間軸を合わせて表示
<details><summary>コード</summary>
  
```python

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from os.path import join
import mne

from mne.decoding import ReceptiveField
from sklearn.model_selection import KFold
from sklearn.preprocessing import scale
import pandas as pd

# 前処理あり
# EEGデータのパス
eeg_path = "/Users/ami/Desktop/UCSD/prepro_10.fif"

# 顔表情特徴量のCSVファイルパス
face_path = "/Users/ami/Desktop/UCSD/result_mix/10/out_extract_10/extracted_data10_02.csv"

# EEGデータの読み込み
raw = mne.io.read_raw_fif(eeg_path, preload=True)
sfreq = raw.info['sfreq'] # サンプリング周波数を取得
n_channels = len(raw.ch_names) # チャンネル数を取得

# 顔表情特徴量の読み込み
face_data = pd.read_csv(face_path)
face = face_data.iloc[:, 15]  # 16列目(Lips part)
print(face)

# ダウンサンプリング
#decim = 2
#raw.resample(sfreq / decim)

# 242.560546875秒後から60秒間のデータを抽出 ()
start_time = 242.560546875
end_time = start_time + 60
raw.crop(tmin=start_time, tmax=end_time) # 指定した時間帯のデータを抽出

# EEGデータのモンタージュ作成
montage = mne.channels.make_standard_montage("biosemi32")

# チャンネル数を取得
n_channels = len(montage.ch_names) # モンタージュのチャンネル数を取得します

# infoを作成
info = mne.create_info(montage.ch_names, sfreq / decim, "eeg")

# RawArrayに渡すデータの長さとチャンネル数を一致させる
data = raw.get_data() # EEGデータを取得
raw = mne.io.RawArray(data[:n_channels, :], info) # データとinfoを合わせて新しいRawArrayを作成

# プロット
fig, ax = plt.subplots()
lns = ax.plot(scale(raw[:, :60][0].T), color="b", alpha=0.2) # EEGデータをプロットします
ln1 = ax.plot(scale(face[:60]), color="r", lw=2)  #顔表情特徴量をプロットします
ax.legend([lns[0], ln1[0]], ["EEG", "face"], frameon=False)
ax.set(title="Sample activity", xlabel="Time (s)")
mne.viz.tight_layout()
plt.show()

# スケーリングは、データの平均が0になり、標準偏差が1になるように変換される
# この変換により、データの単位や分布の差異に関係なく
# 異なるデータソースのデータを比較しやすくなり、グラフにプロットすることができる
```  
</details>


脳波と特徴量の相関を計算
<details><summary>コード</summary>
  
```python

#mTRFツールボックスと同様のことが、mne.decoding.ReceptiveFieldクラスでできる
#mne.decoding.ReceptiveFieldは、時間遅延を考慮した入力特徴量を使用して、
#エンコーディングモデル（刺激から脳へのモデル）またはデコーディングモデル（脳から刺激へのモデル）を適合させるためのクラス
#これにより、例えばスペクトログラムや時空間的な受容野（STRF）などの時間遅延入力特徴量を使用して、
#脳活動と外部刺激の関係を理解し、予測することができる

# Receptive Field（受容野）で使用する遅延（delay）を定義
tmin, tmax = -0.2, 0.4

# ReceptiveFieldモデルを初期化
#指定した時間範囲（tminからtmaxまで）、サンプリング周波数（sfreq）を持ち、特徴量名とスコアリング方法を設定
#feature_names: モデルの入力特徴量の名前（オプション）。指定しない場合、fitを実行した後に入力データの形状から自動生成
#estimator: モデルの適合に使用する推定器（scikit-learnスタイルのモデル）またはRidge回帰モデルのアルファパラメータ。Noneの場合、Ridge回帰モデル（アルファ=0）が使用される
rf = ReceptiveField(
    tmin, tmax, sfreq, feature_names=["envelope"], estimator=1.0, scoring="corrcoef"
)

# (tmax - tmin) * sfreq の遅延
# 開始/終了インデックスも含むため、追加で2つの遅延がある
n_delays = int((tmax - tmin) * sfreq) + 2

# 交差検証のための分割数を設定し、KFoldクラスを初期化
n_splits = 3
cv = KFold(n_splits)

# モデルのデータを準備。faceデータを転置し、モデルの出力データYを取得。
face = face.T
Y, _ = raw[:]  # Outputs for the model
Y = Y.T

# スプリットごとにモデルを適合させ、予測/テストを繰り返す
coefs = np.zeros((n_splits, n_channels, n_delays))
scores = np.zeros((n_splits, n_channels))
for ii, (train, test) in enumerate(cv.split(face)):
    print("split %s / %s" % (ii + 1, n_splits))
    
    X_train = face[train][:, np.newaxis]  # n_featuresのために新しい軸を追加
    
    # モデルを適合
    rf.fit(X_train, Y[train])
    
    # 同じ形状のテストデータを準備
    X_test = face[test][:, np.newaxis]
    
    # スコアと係数を計算
    scores[ii] = rf.score(X_test, Y[test])
    coefs[ii] = rf.coef_[:, 0, :]

# 遅延の配列を計算
delays = np.linspace(tmin, tmax, n_delays)

times = np.arange(n_delays) * (1.0 / sfreq)

# 交差検証スプリットごとにスコアと係数を平均化
mean_coefs = coefs.mean(axis=0)
mean_scores = scores.mean(axis=0)

# 平均予測スコアをプロット
fig, ax = plt.subplots()
ix_chs = np.arange(n_channels)
ax.plot(ix_chs, mean_scores)
ax.axhline(0, ls="--", color="r")
ax.set(title="Mean prediction score", xlabel="Channel", ylabel="Score ($r$)")
mne.viz.tight_layout()
plt.show()

#縦の値は相関係数
#この相関係数は、脳活動と外部刺激がどれだけ同期しているかを示す指標であり、
#高い相関係数は、脳活動が外部刺激に対して敏感であることを示す
```  
</details>

遅延を考慮して、mapを作成
<details><summary>コード</summary>
  
```python

# 'times' 配列を 'mean_coefs' の列数に合わせて切り詰める。これは後のグラフの作成に使用される。
#?
times = times[:len(mean_coefs[0])]

# Print mean coefficients across all time delays / channels (see Fig 1)
time_plot = 0.180  # For highlighting a specific time.
fig, ax = plt.subplots(figsize=(4, 8))

# 'mean_coefs' 配列内の最大係数を取得します。
max_coef = mean_coefs.max()

# ヒートマップを作成し、係数を視覚化。'times' はX軸、'ix_chs' はY軸、'mean_coefs' は値。
# 'cmap' はカラーマップ、'vmin' および 'vmax' はカラーマップの値の範囲を指定。
ax.pcolormesh(
    times,
    ix_chs,
    mean_coefs,
    cmap="RdBu_r",
    vmin=-max_coef,
    vmax=max_coef,
    shading="gouraud",
)

ax.axvline(time_plot, ls="--", color="k", lw=2)
ax.set(
    xlabel="Delay (s)",
    ylabel="Channel",
    title="Mean Model\nCoefficients",
    xlim=times[[0, -1]],
    ylim=[len(ix_chs) - 1, 0],
    xticks=np.arange(tmin, tmax + 0.2, 0.2),
)

# X軸の目盛りラベルを45度回転
plt.setp(ax.get_xticklabels(), rotation=45)
mne.viz.tight_layout()

# 'times' 配列内で 'time_plot' に最も近い時間を探し、そのインデックスを 'ix_plot' に格納します。
ix_plot = np.argmin(np.abs(time_plot - times))
fig, ax = plt.subplots()

# "biosemi32" テンプレートを使用して Montage オブジェクト 'easycap_montage' を作成
easycap_montage = mne.channels.make_standard_montage("biosemi32")

# チャンネル名、サンプリング周波数、チャンネルタイプを指定して空の 'info' オブジェクトを作成
info = mne.create_info(ch_names=easycap_montage.ch_names, sfreq=1000.0, ch_types='eeg')

#'info' オブジェクトにモンタージュ情報を設定
info.set_montage(easycap_montage)

# マップを作成し、モデルの係数を視覚化。'mean_coefs' の特定の遅延に対する係数が表示される
# 'pos' はセンサーの位置情報、'axes' はグラフ描画のための軸を指定
# 'show=False' はプロットを直接表示しないように指定
mne.viz.plot_topomap(
    mean_coefs[:, ix_plot], pos=info, axes=ax, show=False, vlim=(-max_coef, max_coef)
)
ax.set(title="Topomap of model coefficients\nfor delay %s" % time_plot)
mne.viz.tight_layout()
plt.show()
```
</details>

<strong>被験者10 - 動画1 - 特徴量14(dimple)</strong>

![suteru](https://github.com/am-da/mTRF/blob/main/images/10_1_14.png)



<strong>被験者10 - 動画2 - 特徴量25(Lips part)</strong>

![suteru](https://github.com/am-da/mTRF/blob/main/images/10_2_25.png))


