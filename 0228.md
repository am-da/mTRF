

### 前提
・heatmapは係数の分布  
・topomapでは、係数の正の値の平均値が最大のものを選択   


・coefは係数 (モデルが予測を行う際に各遅延がどれだけの重要性を持つかを示す )   
・coefsの形状は(n_splits, n_channels, n_delays)  

・scoresは相関係数 (各分割でのモデルの評価スコアを保存)  
・scoresの形状は(n_splits, n_channels)  

movie = 40   
features = 17  
subject = 22    

<br> 

<br> 

### biosemi 32 map

<img width="400" alt="スクリーンショット 2024-02-13 15 12 50" src="https://github.com/am-da/mTRF/assets/112613519/be1350ab-e58a-4ed4-b02f-c6484823bbee">   

<br> 

<br> 


## 被験者内map  

 <br> 
 
### subject2に問題あり  

 movie - feature - subject

<details><summary>1. movie1 - subject9のmap (前頭)</summary> 
<img width="906" alt="スクリーンショット 2024-02-27 14 16 31" src="https://github.com/am-da/mTRF/assets/112613519/ef5c019a-e9d4-4bba-8941-b5f89da94737">
</details>
 
 
<details><summary>2. movie1 - subject8のmap (前頭)</summary>

 <br> 
 
1-6-8は特徴量が0であった

<img width="919" alt="スクリーンショット 2024-02-27 14 25 56" src="https://github.com/am-da/mTRF/assets/112613519/466c4c4e-2c09-4ce5-bad4-1a7bd738c318">
</details>


<details><summary>3. movie5 - subject2のmap (⭐️頭頂)</summary>
<img width="893" alt="スクリーンショット 2024-02-27 14 30 58" src="https://github.com/am-da/mTRF/assets/112613519/03a1bb1d-e4f6-4765-a7c0-fdcbb013a729">
</details>


<details><summary>4. movie5 - subject5のmap　(前頭)</summary>
<img width="820" alt="スクリーンショット 2024-02-27 14 33 13" src="https://github.com/am-da/mTRF/assets/112613519/abea8678-f2eb-4fd5-9e97-95efb16f0554">
</details>


<details><summary>5. movie5 - subject17のmap (前頭)</summary>
 <img width="826" alt="スクリーンショット 2024-02-27 14 35 04" src="https://github.com/am-da/mTRF/assets/112613519/8e98c8f3-61bd-4cd7-9b17-997c22c1e85e">
</details>


<details><summary>6. movie19 - subject2のmap (⭐️頭頂)</summary>
<img width="783" alt="スクリーンショット 2024-02-27 14 38 48" src="https://github.com/am-da/mTRF/assets/112613519/ed33f369-294e-4442-8d1c-a2233c343c7f">
</details>


<details><summary>7. movie19 - subject8のmap (前頭)</summary>
<img width="812" alt="スクリーンショット 2024-02-27 14 40 35" src="https://github.com/am-da/mTRF/assets/112613519/ae780622-627f-4f01-84d0-a443ea270e65">
</details>


<details><summary>8. movie14 - subject2のmap (⭐️頭頂)</summary>
<img width="812" alt="スクリーンショット 2024-02-27 14 43 13" src="https://github.com/am-da/mTRF/assets/112613519/4bab7114-2e2f-4bf3-9d9d-cffe384e2da3">
</details>

<details><summary>9. movie5 - subject1のmap (結構まばら)</summary>
<img width="794" alt="スクリーンショット 2024-02-27 14 50 00" src="https://github.com/am-da/mTRF/assets/112613519/5963b9bd-88e6-4b6a-ab26-6ae72972ab6e">
</details>

<details><summary>10. movie32 - subject3のmap (若干頭頂)</summary>
<img width="824" alt="スクリーンショット 2024-02-27 14 53 07" src="https://github.com/am-da/mTRF/assets/112613519/ab65bc28-12c7-4091-82f7-247ddc62a676">
</details>

<details><summary>11. movie5 - subject4のmap (若干頭頂)</summary>
<img width="797" alt="スクリーンショット 2024-02-27 14 55 07" src="https://github.com/am-da/mTRF/assets/112613519/f0388c3b-ef6b-4643-93ef-d01829cdfc37">
</details>

<details><summary>12. movie24 - subject6のmap (前頭部と後頭部に綺麗に分かれている)</summary>
<img width="810" alt="スクリーンショット 2024-02-27 14 58 18" src="https://github.com/am-da/mTRF/assets/112613519/a1899786-d34d-4750-90b4-3bf165265b6d">
</details>

<details><summary>13. movie5 - subject7のmap (まばら)</summary>
<img width="829" alt="スクリーンショット 2024-02-27 15 00 03" src="https://github.com/am-da/mTRF/assets/112613519/0ab6c139-0797-4a07-898c-a5e232d39f9c">
</details>

<details><summary>14. movie5 - subject10のmap (若干頭頂)</summary>
<img width="793" alt="スクリーンショット 2024-02-27 15 02 42" src="https://github.com/am-da/mTRF/assets/112613519/7ce0cca6-76cd-4841-aa06-a317724ac21a">
</details>

<details><summary>15. movie5 - subject11のmap (前頭)</summary>
<img width="806" alt="スクリーンショット 2024-02-27 15 04 38" src="https://github.com/am-da/mTRF/assets/112613519/b96e94b3-bc95-4d35-8bb1-0ac2c3c51a28">
</details>


<details><summary>16. movie25 - subject12のmap (たぶんおかしい. 左右)</summary>
<img width="801" alt="スクリーンショット 2024-02-27 15 09 09" src="https://github.com/am-da/mTRF/assets/112613519/5158b563-bca5-4d72-b809-d6b9965229e2">
</details>


<details><summary>17. movie25 - subject13のmap (結構まばら)</summary>
<img width="797" alt="スクリーンショット 2024-02-27 15 10 54" src="https://github.com/am-da/mTRF/assets/112613519/43040420-318b-49cd-b1aa-4573a2c2ead1">
</details>


<details><summary>18. movie16 - subject14のmap (たぶんおかしい)</summary>
<img width="809" alt="スクリーンショット 2024-02-27 15 12 30" src="https://github.com/am-da/mTRF/assets/112613519/8adeace0-3067-4695-b7bb-aede79786bf6">
</details>


<details><summary>19. movie21 - subject15のmap (前頭)</summary>
<img width="805" alt="スクリーンショット 2024-02-27 15 14 08" src="https://github.com/am-da/mTRF/assets/112613519/0fba9d9e-0ba0-4687-b914-1cab7e49a7b0">
</details>


<details><summary>20. movie30 - subject16のmap (おかしい)</summary>
<img width="775" alt="スクリーンショット 2024-02-27 15 16 15" src="https://github.com/am-da/mTRF/assets/112613519/d203e949-39b9-4ae4-9104-963b5cf2bd57">
</details>

<details><summary>21. movie9 - subject18のmap (おかしい)</summary>
<img width="792" alt="スクリーンショット 2024-02-27 15 18 31" src="https://github.com/am-da/mTRF/assets/112613519/7aff7a12-d7cd-4a85-9796-c96c3225f156">
</details>

<details><summary>22. movie5 - subject19のmap (前頭)</summary>
<img width="807" alt="スクリーンショット 2024-02-27 15 06 36" src="https://github.com/am-da/mTRF/assets/112613519/a525b3b2-41b9-47ad-8617-48dffdaa1cd4">
</details>


<details><summary>23. movie33 - subject20のmap (前頭)</summary>
<img width="837" alt="スクリーンショット 2024-02-27 15 21 40" src="https://github.com/am-da/mTRF/assets/112613519/f29fd787-1630-41ff-b463-a1bf8c57b79d">
</details>


<details><summary>24. movie24 - subject21のmap (微妙)</summary>
<img width="806" alt="スクリーンショット 2024-02-27 15 23 50" src="https://github.com/am-da/mTRF/assets/112613519/f4188813-0525-4d36-a05b-ea6a70cc8ad9">
</details>


<details><summary>25. movie20 - subject22のmap (頭頂)</summary>
<img width="808" alt="スクリーンショット 2024-02-27 15 25 17" src="https://github.com/am-da/mTRF/assets/112613519/ee33f513-3076-457a-a8e1-4b5054ae1ea4">
</details>


<details><summary>特徴量グラフ</summary>
[movie_variance_graph.pdf](https://github.com/am-da/mTRF/files/14415262/movie_variance_graph.pdf)
</details>



 <br> 
 

  

## 被験者平均のmap
 
<details><summary>1. movie5のmap</summary>
 
<br> 

「movie - feature」

<br> 
<img width="1008" alt="スクリーンショット 2024-02-28 6 54 12" src="https://github.com/am-da/mTRF/assets/112613519/549d23e9-4f0e-46b1-b311-0086aa43e25e">
</details>


<details><summary>2. movie5のmap (subject2を除く)</summary> 
<img width="1009" alt="スクリーンショット 2024-02-28 7 05 28" src="https://github.com/am-da/mTRF/assets/112613519/747f411b-ba26-4853-8464-079bed2cea88">
</details>


<details><summary>3. movie5のmap (channel 29 & subject 2 を除く) </summary>
 
<img width="1000" alt="スクリーンショット 2024-02-28 7 18 20" src="https://github.com/am-da/mTRF/assets/112613519/74a2a598-98d8-4b28-b404-75ebe233e59d">

<br> 

### 参考  

2 と 3

 <br> 
 
<img width="1326" alt="スクリーンショット 2024-02-28 7 19 00" src="https://github.com/am-da/mTRF/assets/112613519/6f1c31f0-4a8c-49cf-b52b-6e6a0dc50b0f">
<img width="1355" alt="スクリーンショット 2024-02-28 7 19 24" src="https://github.com/am-da/mTRF/assets/112613519/47ba2432-7cad-4658-9709-a33e92922e73">
</details>



<details><summary>4. movie5のmap (channel 28 & 29 & subject 2 を除く) </summary>
<img width="927" alt="スクリーンショット 2024-02-28 7 33 08" src="https://github.com/am-da/mTRF/assets/112613519/3d4fd5b7-1155-40d8-b6b3-d16b706ad4e0">
</details>


<details><summary>5. movie5のmap (channel 1 & 28 & 29 & subject 2 を除く)</summary>
<img width="806" alt="スクリーンショット 2024-02-28 8 02 59" src="https://github.com/am-da/mTRF/assets/112613519/f827262c-c768-42ab-8f23-d8627b3014cb">
</details>


<details><summary>6. movie5のmap (channel 1 & 2 & 28 & 29 & 32 & subject 2 を除く)</summary>
<img width="905" alt="スクリーンショット 2024-02-28 8 15 44" src="https://github.com/am-da/mTRF/assets/112613519/fde30b77-7777-4879-b0c1-b07c3da7f82b">
<img width="800" alt="スクリーンショット 2024-02-28 8 17 40" src="https://github.com/am-da/mTRF/assets/112613519/a87bf500-0037-43d0-b710-f08a567d99ec">
</details>

<br> 


### 参考
<img width="503" alt="スクリーンショット 2024-02-28 8 07 42" src="https://github.com/am-da/mTRF/assets/112613519/bd670abf-227e-44e1-833f-93609abaff23">



<details><summary>コード</summary>

```python
# グラフ出力成功1
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from os.path import join
import mne

from mne.decoding import ReceptiveField
from sklearn.model_selection import KFold
from sklearn.preprocessing import scale
import pandas as pd

# エクセルファイルからstart_timeを読み込む
start_times_df = f"/Users/ami/PycharmProjects/UCSD_pycharm/UCSD/time_list.csv"
start_times = pd.read_csv(start_times_df)

movie_numbers =  range(5, 6) # 動画の番号 (1~40)
feature_numbers = range(1, 18) # 特徴量17
subject_numbers = range(1, 23) # 被験者数22人

for movie_number in movie_numbers:
    for feature_number in feature_numbers:
        all_raw_data = np.zeros((32, 1921)) # 全ての被験者のデータを格納するための空の配列を作成
        all_face_data = np.zeros(1500) # 全ての被験者の顔データを格納するための空の配列を作成
        for subject_number in subject_numbers:
            print(start_times.iloc[movie_number-1, subject_number])
            start_time = start_times.iloc[movie_number-1, subject_number] # 「注意」movienumber-1　は固定
            print(start_time)
            eeg_path = f"/Users/ami/PycharmProjects/UCSD_pycharm/UCSD/prepro_{subject_number:02d}.fif"
            face_path = f"/Users/ami/PycharmProjects/UCSD_pycharm/UCSD/result_mix/{subject_number}/out_extract_{subject_number:02d}/extracted_data{subject_number:02d}_{movie_number:02d}.csv"
            raw = mne.io.read_raw_fif(eeg_path, preload=True)
            sfreq = raw.info['sfreq']  # サンプリング周波数を取得
            n_channels = len(raw.ch_names) # 変更！
            decim = 2  # (任意の変数)
            sfreq /= decim
            face_data = pd.read_csv(face_path)
            face = face_data.iloc[:, feature_number].values #(1から17)
            face = mne.filter.resample(face.astype(float), down=decim, npad="auto")
            raw = raw.copy().resample(sfreq / decim)  # RawArrayをコピーしてリサンプル
            end_time = start_time + 60
            raw.crop(tmin=start_time, tmax=end_time)  # 指定した時間帯のデータを抽出
            info = mne.create_info(raw.ch_names, sfreq, "eeg") #変更 !
            data = raw.get_data()  # EEGデータを取得
            # data = data[:-1, :]  # 32チャンネル目を除外 変更　！
            print(data.shape)
            all_raw_data += data
            all_face_data += face
        average_raw_data = all_raw_data / 22
        average_face_data = all_face_data / 22
        raw = mne.io.RawArray(average_raw_data, info)  # データとinfoを合わせて新しいRawArrayを作成
        face = average_face_data
        tmin, tmax = -0.5, 0.5
        rf = ReceptiveField(tmin, tmax, sfreq, feature_names=["envelope"], estimator=1.0, scoring="corrcoef")
        n_delays = int((tmax - tmin) * sfreq) + 2
        # 交差検証のための分割数を設定し、KFoldクラスを初期化
        n_splits = 3
        cv = KFold(n_splits)
        # モデルようにデータを準備。faceデータを転置し、モデルの出力データ(EEG)Yを取得。
        face = face.T
        Y, _ = raw[:]
        Y = Y.T
        # 特徴量とEEGの間の線形関係を評価するために、モデルを学習させる
        # スプリットごとにモデルを適合させ、予測/テストを繰り返す
        coefs = np.zeros((n_splits, n_channels, n_delays))
        scores = np.zeros((n_splits, n_channels))
        for ii, (train, test) in enumerate(cv.split(face)):
            print("split %s / %s" % (ii + 1, n_splits))
            X_train = face[train][:, np.newaxis]  # n_featuresのために新しい軸を追加
            rf.fit(X_train, Y[train])
            X_test = face[test][:, np.newaxis]
            scores[ii] = rf.score(X_test, Y[test])
            coefs2 = np.zeros((n_splits, n_channels, n_delays-1))
            coefs2[ii] = rf.coef_[:, 0, :]
        mean_scores = scores.mean(axis=0)
        times = np.linspace(tmin, tmax, n_delays-1)
        # times = np.arange(n_delays) * (1.0 / sfreq)
        # 交差検証スプリットごとのスコアと係数を平均化 coefは係数、scoreは相関係数
        mean_coefs = coefs2.mean(axis=0)
        mean_scores = scores.mean(axis=0)
        # 各遅延時間に対する処理を行います
        positive_sums = []
        positive_counts = []
        # mean_coefs のデータを元に処理を行います
        # mean_coefs が 32x65 の2次元配列として与えられていると仮定します
        # 各遅延時間に対してループを行います
        for i in range(mean_coefs.shape[1]):
            # 各遅延時間における正の値のみを抽出して合計します
            positive_sum = np.sum(mean_coefs[:, i][mean_coefs[:, i] > 0])
            positive_sums.append(positive_sum)
            # 各遅延時間における正の値の個数を数えます
            positive_count = np.sum(mean_coefs[:, i] > 0)
            positive_counts.append(positive_count)
        # 正の値の平均を計算します
        positive_means = [positive_sum / positive_count if positive_count > 0 else 0 for positive_sum, positive_count in zip(positive_sums, positive_counts)]
        # 最も正の平均値が大きい遅延時間を見つけます
        max_positive_mean_index = np.argmax(positive_means)
        max_positive_mean_delay = times[max_positive_mean_index]
        # 結果を出力します
        print("Delay time with maximum positive mean:", max_positive_mean_delay)
        # 平均予測スコアをプロット
        fig, ax = plt.subplots()
        ix_chs = np.arange(n_channels)
        ax.plot(ix_chs, mean_scores)
        ax.axhline(0, ls="--", color="r")
        ax.set(title="Mean prediction score", xlabel="Channel", ylabel="Score ($r$)")
        # plt.tight_layout()
        # Print mean coefficients across all time delays / channels (see Fig 1)
        time_plot = max_positive_mean_delay  # For highlighting a specific time.
        fig, ax = plt.subplots(figsize=(4, 8))
        max_coef = mean_coefs.max()
        ax.pcolormesh(
            times,
            ix_chs,
            mean_coefs,
            cmap="RdBu_r",
            vmin=-max_coef,
            vmax=max_coef,
            shading="gouraud",
        )
        ax.axvline(time_plot, ls="--", color="k", lw=2)
        ax.set(
            xlabel="Delay (s)",
            ylabel="Channel",
            title="Mean Model\nCoefficients",
            xlim=times[[0, -1]],
            ylim=[len(ix_chs) - 1, 0],
            xticks=np.arange(tmin, tmax + 0.2, 0.2),
        )
        plt.setp(ax.get_xticklabels(), rotation=45)
        plt.tight_layout()
        #plt.show()
        plt.savefig(f"/Users/ami/PycharmProjects/UCSD_pycharm/UCSD/output_graph/heatmap_{movie_number}_{feature_number}.png")
        # 'times' 配列内で 'time_plot' に最も近い時間を探し、そのインデックスを 'ix_plot' に格納します。
        ix_plot = np.argmin(np.abs(time_plot - times))
        fig, ax = plt.subplots()
        # "biosemi32" テンプレートを使用して Montage オブジェクト 'easycap_montage' を作成
        easycap_montage = mne.channels.make_standard_montage("biosemi32")
        # チャンネル名、サンプリング周波数、チャンネルタイプを指定して空の 'info' オブジェクトを作成
        info = mne.create_info(ch_names=easycap_montage.ch_names, sfreq=128, ch_types='eeg') # 変更　！
        # print("ch_names")
        # print(easycap_montage.ch_names)
        info.set_montage(easycap_montage)
        mne.viz.plot_topomap(mean_coefs[:, ix_plot], pos=info, axes=ax, show=False, vlim=(-max_coef, max_coef))
        ax.set(title="Topomap of model coefficients\nfor delay %s" % time_plot)
        plt.tight_layout()
        # plt.show()
        plt.savefig(f"/Users/ami/PycharmProjects/UCSD_pycharm/UCSD/output_graph/topomap_{movie_number}_{feature_number}.png")

 
```
</details>








